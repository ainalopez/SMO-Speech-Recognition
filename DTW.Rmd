---
title: "Speech Recognition"
author: "Kseniya Bout, Nick Halliwell, Aina Lopez, Yaroslav Marchuk"
date: "29 March 2016"
output: pdf_document
---

# 1. Introduction

Nick

  
&nbsp;
&nbsp;

# 2. Dynamic Time Warping

Nick  

## 2.1 Algorithm
Nick

## 2.2 Modifications
To inhance performance and sensitivity of the initially proposed algorithm, a couple of modifications have been developed. Among the most significant ones are *the step size conditions*, *the step weighting* and *the global path constraints*.  

The step size condition takes care of possibly unrealistic correspondence between the time-series features of different length by introducing a slope constraint on each step and between several consecutive points of the warping path. For example, after moving in the same direction horizontally for k consecutive points the warping path is not allowed to continue in the same direction before stepping l points in the diagonal direction, as on the figure below.   
![Caption](https://github.com/ainalopez/SMO-Speech-Recognition/blob/master/Figures/sc.png=100x)

Weightning helps to favour or penalize certain types of point-to-point correspondence. Both approaches - symmetric and asymmetric weights - find successful implementation depending on topic researched. Below on the left side of the figure symmetric weights are demonstrated, while on the right side is an asymmetric approach. 
![Caption](https://github.com/ainalopez/SMO-Speech-Recognition/blob/master/Figures/w.png=200x)

Similarly to the step function constraints, the global path constraints are used as the way to improve the computational cost and optimize DTW sensitivity.

## 2.3 Applications

&nbsp;
&nbsp;


# 3. SPEECH RECOGNIZER

We implemented a speech recognizer using a modified version of the DTW algorithm. Using DTW and speech processing techniques (i.e. Mel Frequency Cepstral Coefficients), our algorithm is able to detect which action we want to perform: open the google search or open our facebook webpage. 

This speech recognizer takes as input a sound file and compares it with some template words: *google* and *facebook*. In order to make our recognizer more robust, we have used two template words for each sound. After comparing the input sound with each template word, the algorithm chooses the one with the shortest path. 


## 3.1 Speech Processing

Since each sound has a different length, DTW is a good algorithm for comparing them. However, sound Waves are sinusoids and cannot be properly compared on the time domain (amplitude vs seconds). The reason is simple, a sound wave of a word can change radically from a person to another, and even the sound waves of a word generated by a single person can be very different.  

If we observe the sound waves of our sounds, we see their differences.  


![](Figures\GoogleWaves.png)
*Figure 1. Sound Waves of the word Google.*

![](Figures\FacebookWaves.png)
*Figure 2. Sound Waves of the word Facebook.*   

&nbsp;

An extended approach is to transform the sound waves into the frequency domain and then, delte the unnecessary frequencies and extract some coefficients ([1], [2]). In this project, we tried with two different coefficients: Mel Frequency Cepstral Coefficients (MFCC) and Linear Prediction Coefficient (LPC). Unfortunately, we didn't obtained good results with LPC and we decided to use only MFCC.  
\
\

**1. Pre-emphasizing**: In the process to digitalize audio, it passes through a low-pass filter and the high frequencies are attenuated. In this step a pre-emphasizing filter is applied to emphazise the higest frequenciy components of the speech, that is increase the energy in the higher frequencies.

$$
y(n) = x(n) - 0.95 x(n-1)
$$


**2. Framing**: From the previous images, we can clearly see that sound signals are not stationary. However, we can assume that if we take a small sample called frame (approximately of 10ms or 20 ms) it will be stationary. We created a set of frames of 16ms.  
\
\

**3. Windowing**: In order to make the transition between frames smoother, a window is multiplied with each frame. The most used window for speech procesing is a *Hamming Window*:

$$
w(n) = 0.54 - cos( \frac{2 \pi n}{N-1})
$$  

where N is the length of the window, in this case the length of the frame.  
\
\

**4. Mel Frequency Cepstral Coefficient (MFCC)**: Mel scale is based on human perception of frequencies. The basic idea of MFCC is to only keep the most relevant frequencies and ignore the others. The process is the following:

1. Convert the frames into the frequency domain using the Discrete Fourier Transform (DFT).  

2. Apply the Mel Scale filter bank, which is a set of triangular filters, and take the logarithm.  

3. Finally, to transform the results into the time domain again the Discrete Cosine Transform (DCT) is applied.

From this process we obtain K coefficients, in our case K = 11.  
\
\

At the end, we end up having M frames and each frame contains a vector of 11 coefficients.  


## 3.2 Dynamic Time Warping in this project  
Blablabla

## 3.3 Results
Blablabla


&nbsp;
&nbsp;


# 4. Conclusions
Blablabla


&nbsp;
&nbsp;


# 5. References 

[1] Anand, D. A., & Meher, P. K. Combined LPC and MFCC Features based technique for Isolated Speech Recognition.  

[2] Shinde, M. R., & Pawar, V. P. Dynamic time Warping using MATLAB & PRAAT.  


\pagebreak

# 6. Annex

## 6.1 DTW function 

```{r,echo = TRUE, eval = FALSE}
TimeWarp<-function(x,y,w=4){
  
  # define distance function
  distance<-function(a,b){
    dist(rbind(a,b))
    }
  
  # 1. Compute matrix 11xM
  
  # set parameters
  m<-dim(x)[2]
  n<-dim(y)[2]
  colnames(x)<-1:m
  colnames(y)<-1:n
  w = max(w, abs(n-m))
  
  # Create matrix
  DTW<-matrix(Inf,n,m)
  rownames(DTW)<-n:1
  colnames(DTW)<-1:m
  
  # Initial values
  DTW['1','1']<-distance(x[,'1'], y[,'1'])

   # First row
  for(j in 2:(w+1)){
    cost<-distance(x[,as.character(j)], y[,as.character(1)])
    DTW['1',as.character(j)]<- cost + DTW['1', as.character(j-1)]
  }
  
   # First column
  for(i in 2:(w+1)){
    cost<-distance(x[,as.character(1)],y[,as.character(i)])
    DTW[as.character(i), '1']<- cost + DTW[as.character(i-1), '1']
  }
  
  # Fill matrix
  for(i in 2:n){
    for(j in (max(2, i-w)):(min(m, i+w))){
      
      #current cost
      cost<-distance(x[,as.character(j)], y[,as.character(i)])
      
      #cumulated cost
      d.cost<-min(DTW[as.character(i-1), as.character(j)] ,
                        DTW[as.character(i), as.character(j-1)], 
                        2*DTW[as.character(i-1), as.character(j-1)])
            
      #combined cost
      DTW[as.character(i),as.character(j)]<-cost + d.cost
    
    }
  }

  # 2. Find path
  path<-matrix(c(n,m), 1,2)
  full.path<-(tail(path,1)[1] ==1 & tail(path,1)[2] ==1)   
  
  while(full.path==FALSE ){
  
    l.path<-tail(path,1)
    
    if(l.path[1]==1 | l.path[2]==1){
      p<-which(l.path==1)
        
          if(p==1){new.point<-c(l.path[1], l.path[2]-1)
           }else{
            new.point<-c(l.path[1]-1, l.path[2])
      }
      
    } else {
    
    # nearest point
    min.step<-min(DTW[as.character(l.path[1]-1), as.character(l.path[2]-1)],
        DTW[as.character(l.path[1]), as.character(l.path[2]-1)],
        DTW[as.character(l.path[1]-1), as.character(l.path[2])])
    min.step<-which(c(DTW[as.character(l.path[1]-1), as.character(l.path[2]-1)],
                    DTW[as.character(l.path[1]), as.character(l.path[2]-1)],
                    DTW[as.character(l.path[1]-1), as.character(l.path[2])])==min.step)
    min.step<-min.step[1]
    
    #path to nearest point
    if(min.step==1){
      new.point<-c(l.path[1]-1, l.path[2]-1)
    } else{
      if(min.step==2){
      new.point<-c(l.path[1], l.path[2]-1)
      } else{
        new.point<-c(l.path[1]-1, l.path[2])  
      }
    }
    }
    path<-rbind(path,new.point)
    full.path<-(tail(path,1)[1] ==1 & tail(path,1)[2] ==1)   
    
    }
  

return(list(path=path, DTW=DTW))
}


```


## 6.2 Speech Recognizer code

```{r, echo = TRUE, eval = FALSE}
# input: isound is the path to the wav file with the sound.

SpeechRecognizer <- function(isound){

  if (!require("tuneR")) install.packages("tuneR");library(tuneR)
  
  # Read the wav file
  sound <- readWave(isound)
  sr    <- sound@samp.rate
  
  # Compute the mel frequency cepstrum coefficients
  inputWord <- t(melfcc(sound, 
                        sr, 
                        wintime=0.016, 
                        lifterexp=0, 
                        minfreq=133.33,
                        maxfreq=6855.6, 
                        sumpower=FALSE))
  
  # Upload the four template sounds and compute their melfcc
  g1 <- readWave("Project\googlel1.wav")
  g2 <- readWave("Project\google2.wav")
  f1 <- readWave("Project\facebook1.wav")
  f2 <- readWave("Project\facebook2.wav")
  
  sr1 <- g1@samp.rate
  sr2 <- g2@samp.rate
  sr3 <- f1@samp.rate
  sr4 <- f2@samp.rate
  
  google1 <- t(melfcc(g1, sr1, wintime=0.016, lifterexp=0, 
                      minfreq=133.33, maxfreq=6855.6, sumpower=FALSE))
  google2 <- t(melfcc(g2, sr2, wintime=0.016, lifterexp=0, 
                      minfreq=133.33, maxfreq=6855.6, sumpower=FALSE))
  facebook1 <- t(melfcc(f1, sr3, wintime=0.016, lifterexp=0, 
                        minfreq=133.33, maxfreq=6855.6, sumpower=FALSE))
  facebook2 <- t(melfcc(b2, sr4, wintime=0.016, lifterexp=0, 
                        minfreq=133.33, maxfreq=6855.6, sumpower=FALSE))
  
  
  # Compute the distance of the input sound with the template sounds
  distance.sound<-rep(NA, 4)
  
  dtwg1 <- TimeWarp(google1, inputWord)
  distance.sound[1]<- tail(dtwg1$DTW[,1],1)

  dtwg2 <- TimeWarp(google2, inputWord)
  distance.sound[2]<- tail(dtwg2$DTW[,1],1)
   
  dtwf1 <- TimeWarp(facebook1, inputWord)
  distance.sound[3]<- tail(dtwf1$DTW[,1],1)
  
  dtwf2 <- TimeWarp(facebook2, inputWord)
  distance.sound[4]<- tail(dtwf2$DTW[,1],1)
  
  
  # If the minimum distance is to the word gmail, open gmail
  if (which.min(distance.sound) == 1 | which.min(distance.sound) == 2){
    system(paste("open http://google.com"))
  } 
  
   # If the minimum distance is to the word facebook, open facebook
  if (which.min(distance.sound) ==3 | which.min(distance.sound) == 4){
     system(paste("open http://facebook.com"))
  } 
}
```
